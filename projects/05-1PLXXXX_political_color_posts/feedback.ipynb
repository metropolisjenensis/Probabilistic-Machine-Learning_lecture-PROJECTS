{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4342b00e",
   "metadata": {},
   "source": [
    "# COMMENTS:\n",
    "\n",
    "[23.Mai.2025] \n",
    "- Before starting with classification tasks (like you do well in the 'classification_setup1.ipynb notebook), it is a good practice to provide a description of the dataset. First of all, very general features of the dataset, such as the origin, the meaining, and so on. Next, you can include the number of samples, the number of features, and the number of classes.\n",
    "  More fine-grained information about the dataset can be provided in the form of a table, such as the number of samples per class, missing values, and the number of features per class.\n",
    "  You can start doing that already in the notebook for the report. \n",
    "\n",
    "- Make Histograms in HSV space for trends for different features. You have time information, and it would be interesting to see how differetn observables change over time for each party.  You can think of tracking the evolution of the following observables on the RGB and HSV space:\n",
    "  - Brightness\n",
    "  - Saturation\n",
    "  - Hue\n",
    "  - Colorfulness\n",
    "  - Contrast\n",
    "  - Sharpness\n",
    "  - Dominant color\n",
    "  - Color distribution\n",
    "\n",
    "The ones I'm proposing are only an example of course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34cdb03",
   "metadata": {},
   "source": [
    "[06.Jun.2025]\n",
    "- I have checked your report and it is going very well. I tried XGBoost in your dataset and it is performing very well. I suggest you to try it as well, and compare the results with the ones you have already obtained with Random Forests, SVMs, and MLPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f3692",
   "metadata": {},
   "source": [
    "[27.06.2025] As a last request to consider your project finished, I would like to ask you to repeat this workflow by now using a Gaussian Mixture Model (GMM) instead of K-Means for clustering. This would allow you to compare the results of the \"soft\" clustering made by  GMM with the \"hard\" clustering made by K-Means. You can use the same code you have already written for K-Means, and you will just need to change the clustering algorithm to use GMM. \n",
    "\n",
    "I am asking you this not to annoy you, but to invite you to explore the dataset further and to see how different clustering algorithms can yield different insights. \n",
    "\n",
    "Moreover: K-means is a special case of GMM, and it is not inherently a probabilistic method. GMM, on the other hand, allows for soft clustering, where each data point can belong to multiple clusters with different probabilities (called responsibilities). See the lecture slides on GMMs: https://drive.google.com/drive/folders/1c_jzsbBfxUJ4liPvTVpDsGAfjhJK7azM?usp=drive_link \n",
    "\n",
    "They are very easy to implement using the `sklearn.mixture.GaussianMixture` class. \n",
    "\n",
    "With this addition, your project will be really comprehensive and will incorporate a probabilistic approach to clustering, which is one of the goals of the course and a fundamental concept in machine learning. It is a really valuable skill in data science to be able to apply different clustering techniques and understand their implications.\n",
    "\n",
    "Again, this is my final request. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ace354",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
